{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Tutorial: Hyperparameter Tuning with Ray Tune (Detailed Guide)\n",
				"\n",
				"This tutorial explores the automated capabilities of `HyperNOs` for the hyperparameter optimization and tuning of models. We use [Ray Tune](https://docs.ray.io/en/latest/tune/index.html) to automatically search for the best model configuration.\n",
				"\n",
				"> **Prerequisites**: Before getting started, ensure you have set up the environment and downloaded the data. Please refer to the [Installation Guide](README.md#installation) in the main README for details.\n",
				"\n",
				"## Why is this powerful?\n",
				"Unlike simple grid searches, this setup allows you to tune:\n",
				"- **Continuous Parameters**: Learning rates, weight decays (using distributions).\n",
				"- **Discrete Choices**: Activation functions, Architecture types.\n",
				"- **System Parameters**: Batch sizes, number of modes.\n",
				"\n",
				"### Tuning External Libraries\n",
				"Just like the training pipeline, the tuning pipeline is **model-agnostic**. You can use this exact notebook to find the optimal hyperparameters for:\n",
				"- An official model from `neuraloperator` like `TFNO`, `CODANO`, `UNO`, `RNO`, `LocalNO`, `OTNO` and many others.\n",
				"- A model from `deepxde` like `DeepONet`, `MIONet`, `POD-DeepONet`, `POD-MIONet`.\n",
				"- Your own custom `nn.Module` or one of our already implemented models.\n",
				"\n",
				"You simply define the parameters relevant to *that* model in the `config_space`."
			]
		},
		{
			"cell_type": "code",
			"execution_count": 1,
			"id": "e0ecba02",
			"metadata": {},
			"outputs": [
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"/home/max/.pyenv/versions/3.12.7/envs/hypernos/lib/python3.12/site-packages/hyperopt/atpe.py:19: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
						"  import pkg_resources\n"
					]
				}
			],
			"source": [
				"import os\n",
				"import sys\n",
				"import torch\n",
				"from ray import tune\n",
				"\n",
				"# Force local Ray initialization by clearing RAY_ADDRESS if set\n",
				"# This prevents connection timeouts if a stale cluster address is present in the environment\n",
				"if \"RAY_ADDRESS\" in os.environ:\n",
				"    del os.environ[\"RAY_ADDRESS\"]\n",
				"\n",
				"# Ensure the 'neural_operators' package is in the path\n",
				"sys.path.append(\"..\")\n",
				"sys.path.append(os.getcwd())\n",
				"sys.path.append(os.path.join(os.getcwd(), \"..\", \"neural_operators\"))\n",
				"\n",
				"from neural_operators.architectures import FNO\n",
				"from neural_operators.datasets import NO_load_data_model\n",
				"from neural_operators.loss_fun import loss_selector\n",
				"from neural_operators.tune import tune_hyperparameters\n",
				"from neural_operators.utilities import initialize_hyperparameters\n",
				"from neural_operators.wrappers import wrap_model_builder"
			]
		},
		{
			"cell_type": "markdown",
			"id": "640698e0",
			"metadata": {},
			"source": [
				"## 1. Baseline Configuration\n",
				"\n",
				"Before we start tuning, we need a baseline. These values will be used for any parameter that we *don't* explicitly tune. Moreover this configuration will be executed as a first run to get the baseline performance. The obtained result will be used as a reference for the comparison of the tuned models, so we can optimize the optimization process."
			]
		},
		{
			"cell_type": "code",
			"execution_count": 2,
			"id": "e9be4afc",
			"metadata": {},
			"outputs": [],
			"source": [
				"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
				"\n",
				"# Explicit Defaults (Baseline)\n",
				"default_params = {\n",
				"    \"training_samples\": 128,\n",
				"    \"val_samples\": 64,\n",
				"    \"test_samples\": 64,\n",
				"    \"learning_rate\": 0.001,\n",
				"    \"epochs\": 5, \n",
				"    \"batch_size\": 32,\n",
				"    \"weight_decay\": 1e-4,\n",
				"    \"beta\": 1,\n",
				"    \"scheduler_step\": 23,\n",
				"    \"scheduler_gamma\": 0.86,\n",
				"    # ... FNO specific args ...\n",
				"    \"width\": 32,\n",
				"    \"modes\": 8,\n",
				"    \"n_layers\": 2,\n",
				"    \"padding\": 10,\n",
				"    \"fno_arc\": \"Residual\",\n",
				"    \"fun_act\": \"gelu\",\n",
				"    \"in_dim\": 1,\n",
				"    \"out_dim\": 1,\n",
				"    \"fft_norm\": None,\n",
				"    \"FourierF\": 0,\n",
				"    \"RNN\": False,\n",
				"    \"include_grid\": 1,\n",
				"    \"weights_norm\": \"Kaiming\",\n",
				"    \"retrain\": 4,\n",
				"    \"problem_dim\": 2,\n",
				"    \"filename\": None\n",
				"}"
			]
		},
		{
			"cell_type": "markdown",
			"id": "e1f7c643",
			"metadata": {},
			"source": [
				"## 2. Defining the Search Space\n",
				"\n",
				"This is the core of the experiment. We define a dictionary where keys are parameter names and values are **distributions**.\n",
				"\n",
				"- `tune.choice([A, B])`: Randomly pick A or B.\n",
				"- `tune.uniform(min, max)`: Uniform float sampling.\n",
				"- `tune.quniform(min, max, q)`: Quantized uniform sampling (good for discrete steps like hidden units).\n",
				"- `tune.randint(min, max)`: Random integer."
			]
		},
		{
			"cell_type": "code",
			"execution_count": 3,
			"id": "1203d71a",
			"metadata": {},
			"outputs": [],
			"source": [
				"config_space = {\n",
				"    # --- Optimization Tuning ---\n",
				"    # Explore learning rates between 0.0001 and 0.01\n",
				"    \"learning_rate\": tune.quniform(1e-4, 1e-2, 1e-5),\n",
				"    # Explore weight decay regularization\n",
				"    \"weight_decay\": tune.quniform(1e-6, 1e-3, 1e-6),\n",
				"    \n",
				"    # --- Architecture Tuning ---\n",
				"    # Try different network widths (channel capacity)\n",
				"    \"width\": tune.choice([16, 32]),\n",
				"    # Try different depths\n",
				"    \"n_layers\": tune.randint(2, 4),\n",
				"    # Try different number of Fourier modes\n",
				"    \"modes\": tune.choice([8, 12]),\n",
				"    \n",
				"    # --- Component Tuning ---\n",
				"    # Compare Activation functions\n",
				"    \"fun_act\": tune.choice([\"gelu\", \"relu\"])\n",
				"}\n",
				"\n",
				"# Merge strategy: Take defaults, overwrite with search space keys\n",
				"fixed_params = default_params.copy()\n",
				"for param in config_space.keys():\n",
				"    fixed_params.pop(param, None)\n",
				"\n",
				"config_space.update(fixed_params)\n",
				"\n",
				"# Start the search with our known 'best' default (Optional but recommended)\n",
				"default_hyper_params = [default_params]"
			]
		},
		{
			"cell_type": "markdown",
			"id": "316646c1",
			"metadata": {},
			"source": [
				"## 3. The Builder Interface\n",
				"\n",
				"The `tune_hyperparameters` function works by calling this builder for *every trial* with a new `config` sampled from the space above.\n",
				"\n",
				"This allows for dynamic graph construction. For example, if `config[\"n_layers\"]` changes, the model effectively grows or shrinks."
			]
		},
		{
			"cell_type": "code",
			"execution_count": 4,
			"id": "c42d7d9e",
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"ðŸ“‚ File Darcy_64x64_IN.h5 found in /home/max/Desktop/HyperNOs/data\n",
						"ðŸ“‚ File Darcy_64x64_IN.h5 found in /home/max/Desktop/HyperNOs/data\n",
						"ðŸ“‚ File Darcy_64x64_IN.h5 found in /home/max/Desktop/HyperNOs/data\n"
					]
				}
			],
			"source": [
				"which_example = \"darcy\"\n",
				"\n",
				"# Just to get the output normalizer\n",
				"dummy_example = NO_load_data_model(\n",
				"    which_example=which_example,\n",
				"    no_architecture={\n",
				"        \"FourierF\": default_hyper_params[0][\"FourierF\"],\n",
				"        \"retrain\": default_hyper_params[0][\"retrain\"],\n",
				"    },\n",
				"    batch_size=default_hyper_params[0][\"batch_size\"],\n",
				"    training_samples=default_hyper_params[0][\"training_samples\"],\n",
				"    filename=default_hyper_params[0].get(\"filename\", None),\n",
				")\n",
				"\n",
				"# Safely get output_normalizer (some datasets like Darcy don't have it)\n",
				"output_normalizer = getattr(dummy_example, \"output_normalizer\", None)\n",
				"\n",
				"def builder(config):\n",
				"    return FNO(\n",
				"        config[\"problem_dim\"],\n",
				"        config[\"in_dim\"],\n",
				"        config[\"width\"],\n",
				"        config[\"out_dim\"],\n",
				"        config[\"n_layers\"],\n",
				"        config[\"modes\"],\n",
				"        config[\"fun_act\"],\n",
				"        config[\"weights_norm\"],\n",
				"        config[\"fno_arc\"],\n",
				"        config[\"RNN\"],\n",
				"        config[\"fft_norm\"],\n",
				"        config[\"padding\"],\n",
				"        device,\n",
				"        (\n",
				"            output_normalizer\n",
				"            if (\"internal_normalization\" in config and config[\"internal_normalization\"])\n",
				"            else None\n",
				"        ),\n",
				"        config[\"retrain\"],\n",
				"    )\n",
				"\n",
				"model_builder = wrap_model_builder(builder, which_example)\n",
				"\n",
				"dataset_builder = lambda config: NO_load_data_model(\n",
				"    which_example=which_example,\n",
				"    no_architecture={\n",
				"        \"FourierF\": config.get(\"FourierF\", 0),\n",
				"        \"retrain\": config.get(\"retrain\", 42),\n",
				"    },\n",
				"    batch_size=config[\"batch_size\"],\n",
				"    training_samples=config[\"training_samples\"],\n",
				"    filename=config.get(\"filename\", None),\n",
				")"
			]
		},
		{
			"cell_type": "markdown",
			"id": "5d8d6210",
			"metadata": {},
			"source": [
				"## 4. Launching the Experiment\n",
				"\n",
				"We use `tune_hyperparameters` to kick off the Ray session. This will:\n",
				"1.  Allocate resources (`runs_per_cpu`, `runs_per_gpu`).\n",
				"2.  Schedule parallel trials.\n",
				"3.  Log metrics (Loss across epochs).\n",
				"\n",
				"Results will be saved in `../tests/<experiment_name>`."
			]
		},
		{
			"cell_type": "code",
			"execution_count": 5,
			"id": "773b763b",
			"metadata": {},
			"outputs": [
				{
					"data": {
						"text/html": [
							"<div class=\"tuneStatus\">\n",
							"  <div style=\"display: flex;flex-direction: row\">\n",
							"    <div style=\"display: flex;flex-direction: column;\">\n",
							"      <h3>Tune Status</h3>\n",
							"      <table>\n",
							"<tbody>\n",
							"<tr><td>Current time:</td><td>2026-01-18 18:21:49</td></tr>\n",
							"<tr><td>Running for: </td><td>00:00:10.81        </td></tr>\n",
							"<tr><td>Memory:      </td><td>10.9/30.5 GiB      </td></tr>\n",
							"</tbody>\n",
							"</table>\n",
							"    </div>\n",
							"    <div class=\"vDivider\"></div>\n",
							"    <div class=\"systemInfo\">\n",
							"      <h3>System Info</h3>\n",
							"      Using AsyncHyperBand: num_stopped=2<br>Bracket: Iter 2.000: -0.36809404380619526<br>Logical resource usage: 5.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
							"    </div>\n",
							"    \n",
							"  </div>\n",
							"  <div class=\"hDivider\"></div>\n",
							"  <div class=\"trialStatus\">\n",
							"    <h3>Trial Status</h3>\n",
							"    <table>\n",
							"<thead>\n",
							"<tr><th>Trial name       </th><th>status    </th><th>loc                   </th><th style=\"text-align: right;\">  FourierF</th><th>RNN  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  beta</th><th style=\"text-align: right;\">  epochs</th><th>fft_norm  </th><th>filename  </th><th>fno_arc  </th><th>fun_act  </th><th style=\"text-align: right;\">  in_dim</th><th style=\"text-align: right;\">  include_grid</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  modes</th><th style=\"text-align: right;\">  n_layers</th><th style=\"text-align: right;\">  out_dim</th><th style=\"text-align: right;\">  padding</th><th style=\"text-align: right;\">  problem_dim</th><th style=\"text-align: right;\">  retrain</th><th style=\"text-align: right;\">  scheduler_gamma</th><th style=\"text-align: right;\">  scheduler_step</th><th style=\"text-align: right;\">  test_samples</th><th style=\"text-align: right;\">  training_samples</th><th style=\"text-align: right;\">  val_samples</th><th style=\"text-align: right;\">  weight_decay</th><th>weights_norm  </th><th style=\"text-align: right;\">  width</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  relative_loss</th></tr>\n",
							"</thead>\n",
							"<tbody>\n",
							"<tr><td>train_fn_51fd4646</td><td>TERMINATED</td><td>192.167.74.248:2980526</td><td style=\"text-align: right;\">         0</td><td>False</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       5</td><td>          </td><td>          </td><td>Residual </td><td>gelu     </td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">             1</td><td style=\"text-align: right;\">        0.001  </td><td style=\"text-align: right;\">      8</td><td style=\"text-align: right;\">         2</td><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">       10</td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">        4</td><td style=\"text-align: right;\">             0.86</td><td style=\"text-align: right;\">              23</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">               128</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">      0.0001  </td><td>Kaiming       </td><td style=\"text-align: right;\">     32</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         2.52928</td><td style=\"text-align: right;\">       0.228778</td></tr>\n",
							"<tr><td>train_fn_042e3340</td><td>TERMINATED</td><td>192.167.74.248:2980649</td><td style=\"text-align: right;\">         0</td><td>False</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       5</td><td>          </td><td>          </td><td>Residual </td><td>gelu     </td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">             1</td><td style=\"text-align: right;\">        0.00214</td><td style=\"text-align: right;\">      8</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">       10</td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">        4</td><td style=\"text-align: right;\">             0.86</td><td style=\"text-align: right;\">              23</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">               128</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">      0.000191</td><td>Kaiming       </td><td style=\"text-align: right;\">     16</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         2.50718</td><td style=\"text-align: right;\">       0.190539</td></tr>\n",
							"</tbody>\n",
							"</table>\n",
							"  </div>\n",
							"</div>\n",
							"<style>\n",
							".tuneStatus {\n",
							"  color: var(--jp-ui-font-color1);\n",
							"}\n",
							".tuneStatus .systemInfo {\n",
							"  display: flex;\n",
							"  flex-direction: column;\n",
							"}\n",
							".tuneStatus td {\n",
							"  white-space: nowrap;\n",
							"}\n",
							".tuneStatus .trialStatus {\n",
							"  display: flex;\n",
							"  flex-direction: column;\n",
							"}\n",
							".tuneStatus h3 {\n",
							"  font-weight: bold;\n",
							"}\n",
							".tuneStatus .hDivider {\n",
							"  border-bottom-width: var(--jp-border-width);\n",
							"  border-bottom-color: var(--jp-border-color0);\n",
							"  border-bottom-style: solid;\n",
							"}\n",
							".tuneStatus .vDivider {\n",
							"  border-left-width: var(--jp-border-width);\n",
							"  border-left-color: var(--jp-border-color0);\n",
							"  border-left-style: solid;\n",
							"  margin: 0.5em 1em 0.5em 1em;\n",
							"}\n",
							"</style>\n"
						],
						"text/plain": [
							"<IPython.core.display.HTML object>"
						]
					},
					"metadata": {},
					"output_type": "display_data"
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"\u001b[36m(pid=2980526)\u001b[0m /home/max/.pyenv/versions/3.12.7/envs/hypernos/lib/python3.12/site-packages/hyperopt/atpe.py:19: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
						"\u001b[36m(pid=2980526)\u001b[0m   import pkg_resources\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\u001b[36m(train_fn pid=2980526)\u001b[0m ðŸ“‚ File Darcy_64x64_IN.h5 found in /home/max/Desktop/HyperNOs/data\n",
						"\u001b[36m(train_fn pid=2980526)\u001b[0m ðŸ“‚ File Darcy_64x64_IN.h5 found in /home/max/Desktop/HyperNOs/data\n",
						"\u001b[36m(train_fn pid=2980526)\u001b[0m ðŸ“‚ File Darcy_64x64_IN.h5 found in /home/max/Desktop/HyperNOs/data\n",
						"\u001b[36m(train_fn pid=2980526)\u001b[0m PyTorch compilation enabled for better performance\n",
						"\u001b[36m(train_fn pid=2980526)\u001b[0m No wrapper defined for darcy, returning the original model\n",
						"\u001b[36m(train_fn pid=2980526)\u001b[0m Total Parameters: 535,393\n"
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"\u001b[36m(train_fn pid=2980526)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/max/ray_results/train_fn_2026-01-18_18-21-39/trial_51fd4646/checkpoint_000000)\n",
						"\u001b[36m(train_fn pid=2980526)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/max/ray_results/train_fn_2026-01-18_18-21-39/trial_51fd4646/checkpoint_000001)\n",
						"\u001b[36m(pid=2980649)\u001b[0m /home/max/.pyenv/versions/3.12.7/envs/hypernos/lib/python3.12/site-packages/hyperopt/atpe.py:19: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
						"\u001b[36m(pid=2980649)\u001b[0m   import pkg_resources\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\u001b[36m(train_fn pid=2980649)\u001b[0m ðŸ“‚ File Darcy_64x64_IN.h5 found in /home/max/Desktop/HyperNOs/data\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
						"\u001b[36m(train_fn pid=2980649)\u001b[0m PyTorch compilation enabled for better performance\n",
						"\u001b[36m(train_fn pid=2980649)\u001b[0m No wrapper defined for darcy, returning the original model\n",
						"\u001b[36m(train_fn pid=2980649)\u001b[0m Total Parameters: 202,305\n"
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"\u001b[36m(train_fn pid=2980649)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/max/ray_results/train_fn_2026-01-18_18-21-39/trial_042e3340/checkpoint_000000)\n",
						"2026-01-18 18:21:49,998\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/max/ray_results/train_fn_2026-01-18_18-21-39' in 0.0037s.\n",
						"2026-01-18 18:21:50,001\tINFO tune.py:1041 -- Total run time: 10.82 seconds (10.80 seconds for the tuning loop).\n"
					]
				}
			],
			"source": [
				"loss_fn_str = \"L2\"\n",
				"\n",
				"loss_fn = loss_selector(\n",
				"    loss_fn_str=loss_fn_str,\n",
				"    problem_dim=config_space[\"problem_dim\"],\n",
				"    beta=config_space[\"beta\"],\n",
				")\n",
				"\n",
				"best_result = tune_hyperparameters(\n",
				"    config_space,\n",
				"    model_builder,\n",
				"    dataset_builder,\n",
				"    loss_fn,\n",
				"    default_hyper_params,\n",
				"    # --- Resource Allocation ---\n",
				"    # Adjust these based on your machine's logical cores and GPUs\n",
				"    # If float, can run multiple trials per device (e.g. 0.5 = 2 runs per GPU)\n",
				"    runs_per_cpu=5.0, # number of logical cores of your CPU\n",
				"    runs_per_gpu=1.0, # fraction of GPU allocated per trial\n",
				"    \n",
				"    # --- Tuning Budget ---\n",
				"    # Total number of trials (combinations) to sample from the config_space\n",
				"    num_samples=2,\n",
				"    \n",
				"    # Maximum training epochs per trial\n",
				"    max_epochs=5,\n",
				"    \n",
				"    # --- Early Stopping (ASHA Scheduler) ---\n",
				"    # Grace period: Minimum epochs to run before considering stopping a trial\n",
				"    grace_period=2,\n",
				"    \n",
				"    # Reduction factor: Quantifies how aggressive the pruning is.\n",
				"    # E.g., 4 means only top 1/4 of trials are kept after each run.\n",
				"    reduction_factor=4,\n",
				"    \n",
				"    # --- Checkpointing ---\n",
				"    # Frequency (in epochs) to save model state\n",
				"    checkpoint_freq=10,\n",
				")"
			]
		},
		{
			"cell_type": "markdown",
			"id": "02c59187",
			"metadata": {},
			"source": [
				"## 5. Analysis of Results\n",
				"\n",
				"After the optimization process, we can inspect the best performing configuration."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "e208c666",
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Best hyperparameters found were:  {'learning_rate': 0.00214, 'weight_decay': 0.00019099999999999998, 'width': 16, 'n_layers': 3, 'modes': 8, 'fun_act': 'gelu', 'training_samples': 128, 'val_samples': 64, 'test_samples': 64, 'epochs': 5, 'batch_size': 32, 'beta': 1, 'scheduler_step': 23, 'scheduler_gamma': 0.86, 'padding': 10, 'fno_arc': 'Residual', 'in_dim': 1, 'out_dim': 1, 'fft_norm': None, 'FourierF': 0, 'RNN': False, 'include_grid': 1, 'weights_norm': 'Kaiming', 'retrain': 4, 'problem_dim': 2, 'filename': None}\n",
						"The best relative loss was:  0.19053876772522926\n"
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"\u001b[36m(train_fn pid=2980649)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/max/ray_results/train_fn_2026-01-18_18-21-39/trial_042e3340/checkpoint_000001)\n"
					]
				}
			],
			"source": [
				"print(\"Best hyperparameters found were: \", best_result.config)\n",
				"\n",
				"if \"relative_loss\" in best_result.metrics:\n",
				"    print(\"The best relative loss was: \", best_result.metrics['relative_loss'])\n",
				"else:\n",
				"    print(\"Metrics available: \", best_result.metrics.keys())"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "95b79d4a",
			"metadata": {},
			"outputs": [],
			"source": []
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "hypernos",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.12.7"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 5
}
