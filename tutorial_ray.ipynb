{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Tutorial: Hyperparameter Tuning with Ray Tune (Detailed Guide)\n",
				"\n",
				"This tutorial explores the automated capabilities of `HyperNOs` for the hyperparameter optimization and tuning of models. We use [Ray Tune](https://docs.ray.io/en/latest/tune/index.html) to automatically search for the best model configuration.\n",
				"\n",
				"> **Prerequisites**: Before getting started, ensure you have set up the environment and downloaded the data. Please refer to the [Installation Guide](README.md#installation) in the main README for details.\n",
				"\n",
				"## Why is this powerful?\n",
				"Unlike simple grid searches, this setup allows you to tune:\n",
				"- **Continuous Parameters**: Learning rates, weight decays (using distributions).\n",
				"- **Discrete Choices**: Activation functions, Architecture types.\n",
				"- **System Parameters**: Batch sizes, number of modes.\n",
				"\n",
				"### Tuning External Libraries\n",
				"Just like the training pipeline, the tuning pipeline is **model-agnostic**. You can use this exact notebook to find the optimal hyperparameters for:\n",
				"- An official model from `neuraloperator` like `TFNO`, `CODANO`, `UNO`, `RNO`, `LocalNO`, `OTNO` and many others.\n",
				"- A model from `deepxde` like `DeepONet`, `MIONet`, `POD-DeepONet`, `POD-MIONet`.\n",
				"- Your own custom `nn.Module` or one of our already implemented models.\n",
				"\n",
				"You simply define the parameters relevant to *that* model in the `config_space`."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"import os\n",
				"import sys\n",
				"import torch\n",
				"from ray import tune\n",
				"\n",
				"# Ensure the 'neural_operators' package is in the path\n",
				"sys.path.append(os.getcwd())\n",
				"sys.path.append(os.path.join(os.getcwd(), \"neural_operators\"))\n",
				"\n",
				"from neural_operators.architectures import FNO\n",
				"from neural_operators.datasets import NO_load_data_model\n",
				"from neural_operators.loss_fun import loss_selector\n",
				"from neural_operators.tune import tune_hyperparameters\n",
				"from neural_operators.utilities import initialize_hyperparameters\n",
				"from neural_operators.wrappers import wrap_model_builder"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## 1. Baseline Configuration\n",
				"\n",
				"Before we start tuning, we need a baseline. These values will be used for any parameter that we *don't* explicitly tune. Moreover this configuration will be executed as a first run to get the baseline performance. The obtained result will be used as a reference for the comparison of the tuned models, so we can optimize the optimization process."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
				"\n",
				"# Explicit Defaults (Baseline)\n",
				"default_params = {\n",
				"    \"training_samples\": 128,\n",
				"    \"val_samples\": 64,\n",
				"    \"test_samples\": 64,\n",
				"    \"learning_rate\": 0.001,\n",
				"    \"epochs\": 5, \n",
				"    \"batch_size\": 32,\n",
				"    \"weight_decay\": 1e-4,\n",
				"    \"beta\": 1,\n",
				"    # ... FNO specific args ...\n",
				"    \"width\": 32,\n",
				"    \"modes\": 8,\n",
				"    \"n_layers\": 2,\n",
				"    \"padding\": 10,\n",
				"    \"fno_arc\": \"Residual\",\n",
				"    \"fun_act\": \"gelu\",\n",
				"    \"in_dim\": 1,\n",
				"    \"out_dim\": 1,\n",
				"    \"fft_norm\": None,\n",
				"    \"FourierF\": 0,\n",
				"    \"RNN\": False,\n",
				"    \"include_grid\": 1,\n",
				"    \"weights_norm\": \"Kaiming\",\n",
				"    \"retrain\": 4,\n",
				"    \"problem_dim\": 2,\n",
				"    \"filename\": None\n",
				"}"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## 2. Defining the Search Space\n",
				"\n",
				"This is the core of the experiment. We define a dictionary where keys are parameter names and values are **distributions**.\n",
				"\n",
				"- `tune.choice([A, B])`: Randomly pick A or B.\n",
				"- `tune.uniform(min, max)`: Uniform float sampling.\n",
				"- `tune.quniform(min, max, q)`: Quantized uniform sampling (good for discrete steps like hidden units).\n",
				"- `tune.randint(min, max)`: Random integer."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"config_space = {\n",
				"    # --- Optimization Tuning ---\n",
				"    # Explore learning rates between 0.0001 and 0.01\n",
				"    \"learning_rate\": tune.quniform(1e-4, 1e-2, 1e-5),\n",
				"    # Explore weight decay regularization\n",
				"    \"weight_decay\": tune.quniform(1e-6, 1e-3, 1e-6),\n",
				"    \n",
				"    # --- Architecture Tuning ---\n",
				"    # Try different network widths (channel capacity)\n",
				"    \"width\": tune.choice([16, 32]),\n",
				"    # Try different depths\n",
				"    \"n_layers\": tune.randint(2, 4),\n",
				"    # Try different number of Fourier modes\n",
				"    \"modes\": tune.choice([8, 12]),\n",
				"    \n",
				"    # --- Component Tuning ---\n",
				"    # Compare Activation functions\n",
				"    \"fun_act\": tune.choice([\"gelu\", \"relu\"])\n",
				"}\n",
				"\n",
				"# Merge strategy: Take defaults, overwrite with search space keys\n",
				"fixed_params = default_params.copy()\n",
				"for param in config_space.keys():\n",
				"    fixed_params.pop(param, None)\n",
				"\n",
				"config_space.update(fixed_params)\n",
				"\n",
				"# Start the search with our known 'best' default (Optional but recommended)\n",
				"default_hyper_params = [default_params]"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## 3. The Builder Interface\n",
				"\n",
				"The `tune_hyperparameters` function works by calling this builder for *every trial* with a new `config` sampled from the space above.\n",
				"\n",
				"This allows for dynamic graph construction. For example, if `config[\"n_layers\"]` changes, the model effectively grows or shrinks."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"which_example = \"darcy\"\n",
				"\n",
				"# Just to get the output normalizer\n",
				"dummy_example = NO_load_data_model(\n",
				"    which_example=which_example,\n",
				"    no_architecture={\n",
				"        \"FourierF\": default_hyper_params[0][\"FourierF\"],\n",
				"        \"retrain\": default_hyper_params[0][\"retrain\"],\n",
				"    },\n",
				"    batch_size=default_hyper_params[0][\"batch_size\"],\n",
				"    training_samples=default_hyper_params[0][\"training_samples\"],\n",
				"    filename=default_hyper_params[0].get(\"filename\", None),\n",
				")\n",
				"\n",
				"def builder(config):\n",
				"    return FNO(\n",
				"        config[\"problem_dim\"],\n",
				"        config[\"in_dim\"],\n",
				"        config[\"width\"],\n",
				"        config[\"out_dim\"],\n",
				"        config[\"n_layers\"],\n",
				"        config[\"modes\"],\n",
				"        config[\"fun_act\"],\n",
				"        config[\"weights_norm\"],\n",
				"        config[\"fno_arc\"],\n",
				"        config[\"RNN\"],\n",
				"        config[\"fft_norm\"],\n",
				"        config[\"padding\"],\n",
				"        device,\n",
				"        (\n",
				"            dummy_example.output_normalizer\n",
				"            if (\"internal_normalization\" in config and config[\"internal_normalization\"])\n",
				"            else None\n",
				"        ),\n",
				"        config[\"retrain\"],\n",
				"    )\n",
				"\n",
				"model_builder = wrap_model_builder(builder, which_example)\n",
				"\n",
				"dataset_builder = lambda config: NO_load_data_model(\n",
				"    which_example=which_example,\n",
				"    no_architecture={\n",
				"        \"FourierF\": config.get(\"FourierF\", 0),\n",
				"        \"retrain\": config.get(\"retrain\", 42),\n",
				"    },\n",
				"    batch_size=config[\"batch_size\"],\n",
				"    training_samples=config[\"training_samples\"],\n",
				"    filename=config.get(\"filename\", None),\n",
				")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## 4. Launching the Experiment\n",
				"\n",
				"We use `tune_hyperparameters` to kick off the Ray session. This will:\n",
				"1.  Allocate resources (`runs_per_cpu`, `runs_per_gpu`).\n",
				"2.  Schedule parallel trials.\n",
				"3.  Log metrics (Loss across epochs).\n",
				"\n",
				"Results will be saved in `../tests/<experiment_name>`."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "773b763b",
			"metadata": {},
			"outputs": [],
			"source": [
				"loss_fn_str = \"L2\"\n",
				"\n",
				"loss_fn = loss_selector(\n",
				"    loss_fn_str=loss_fn_str,\n",
				"    problem_dim=config_space[\"problem_dim\"],\n",
				"    beta=config_space[\"beta\"],\n",
				")\n",
				"\n",
				"tune_hyperparameters(\n",
				"    config_space,\n",
				"    model_builder,\n",
				"    dataset_builder,\n",
				"    loss_fn,\n",
				"    default_hyper_params,\n",
				"    # --- Resource Allocation ---\n",
				"    # Adjust these based on your machine's logical cores and GPUs\n",
				"    # If float, can run multiple trials per device (e.g. 0.5 = 2 runs per GPU)\n",
				"    runs_per_cpu=5.0, # number of logical CPU's cores allocated per trial\n",
				"    runs_per_gpu=1.0, # fraction of GPU allocated per trial\n",
				"    \n",
				"    # --- Tuning Budget ---\n",
				"    # Total number of trials (combinations) to sample from the config_space\n",
				"    num_samples=20,\n",
				"    \n",
				"    # Maximum training epochs per trial\n",
				"    max_epochs=5,\n",
				"    \n",
				"    # --- Early Stopping (ASHA Scheduler) ---\n",
				"    # Grace period: Minimum epochs to run before considering stopping a trial\n",
				"    grace_period=2,\n",
				"    \n",
				"    # Reduction factor: Quantifies how aggressive the pruning is.\n",
				"    # E.g., 4 means only top 1/4 of trials are kept after each run.\n",
				"    reduction_factor=4,\n",
				"    \n",
				"    # --- Checkpointing ---\n",
				"    # Frequency (in epochs) to save model state\n",
				"    checkpoint_freq=10,\n",
				")"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Python 3",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.8.10"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 5
}
